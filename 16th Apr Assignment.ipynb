{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bda32591-baa6-485f-a161-0d9d9b085365",
   "metadata": {},
   "source": [
    "# 16th April Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46a533d-8d1a-47c8-9902-e8c3be1c7fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d07deb6b-62d5-4e27-afd6-3e10cea8711f",
   "metadata": {},
   "source": [
    "## Q1. What is boosting in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565067f2-49bf-4ee9-b6a7-f608f9f78570",
   "metadata": {},
   "source": [
    "- > Boosting is type of ensemble technique.\n",
    "\n",
    "- > We sequentially train multiple decision tree models in this technique.\n",
    "\n",
    "- > In boosting technique we combine prediction of multiple weak learners to create strong learner.\n",
    "\n",
    "* These are some boosting techniques.\n",
    "\n",
    "1. Adaboost\n",
    "\n",
    "2. Gradiantboost\n",
    "\n",
    "3. Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d8ba66-7407-4cb7-8242-1c52569596f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "939c7a72-29c8-497f-8b53-fc9529896ea2",
   "metadata": {},
   "source": [
    "## Q2. What are the advantages and limitations of using boosting techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da4e06c-63ad-498c-9e7b-381c3c8d77e7",
   "metadata": {},
   "source": [
    "* Advantages\n",
    "\n",
    "We can achieve high prediction as compare to individual weak learner.\n",
    "\n",
    "Boosting technique are less prone to overfitting as compare to weak learner.\n",
    "\n",
    "It is robust to outliers.\n",
    "\n",
    "Boosting technique can be applied to different types of data and task.\n",
    "\n",
    "* Disadvantages\n",
    "\n",
    "Training a boosting models is computationally more expensive and complex than single weak learner.\n",
    "\n",
    "Boosting algorithms have hyperparameters that need to be tuned for optimal performance. \n",
    "\n",
    "While boosting reduces overfitting in general, if the number of iterations is too high or if the learning rate is too low, there is a risk of overfitting the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f037e2f-6582-43d4-a366-825cbc0759ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d09e9338-5e89-4571-8fd3-4a0572c8d99d",
   "metadata": {},
   "source": [
    "## Q3. Explain how boosting works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dfbef4-7550-43bd-91e5-1af009ae5052",
   "metadata": {},
   "source": [
    "Step - 1 : We will assigne equal sample weight to every instances of the dataset.\n",
    "\n",
    "Step - 2 : After that we will calculate the sum of error.Sum of error is sum of sample weights of misclassified samples.\n",
    "\n",
    "Step - 3 : We will calculate the performance of stump with help of sum of error.\n",
    "\n",
    "Step - 4 : With the help of performance of stump we will update the sample weight.\n",
    "\n",
    "Step - 5 : We have to normalize the weights.\n",
    "\n",
    "Step - 6 : We will assigne bin with the help of normalized weight.\n",
    "\n",
    "Step - 7 : Model will generate random number between 0 to 1 and in which bin that number will lie that perticular instance will send to the next model for training.\n",
    "\n",
    "Step - 8 : Formula : ∑ (lambda(Yes))\n",
    "\n",
    "               : ∑ (lambda(No))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f92d0-a154-499c-b428-d5c749626d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "deaea075-d733-49eb-96c9-37dcf1d865b0",
   "metadata": {},
   "source": [
    "## Q4. What are the different types of boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e78100-a2c8-4afa-ab0e-a775fa14481c",
   "metadata": {},
   "source": [
    "1. Adaboost\n",
    "\n",
    "2. Gradiantboost\n",
    "\n",
    "3. XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423eeea1-8842-44c2-a496-8306c4cfed0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9654ff03-0cfa-4466-a9f3-d7897745f1c9",
   "metadata": {},
   "source": [
    "## Q5. What are some common parameters in boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d74766-b1bb-4aa2-a819-fc0d414f184e",
   "metadata": {},
   "source": [
    "No. of trees\n",
    "\n",
    "Learning Rate\n",
    "\n",
    "Maximum depth of tree\n",
    "\n",
    "Minimum samples for split\n",
    "\n",
    "Subsamples\n",
    "\n",
    "Los Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2bbf04-05b7-4791-8b71-0e928ae86029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2eaec02-09bc-462e-977e-53f9c08629ea",
   "metadata": {},
   "source": [
    "## Q6. How do boosting algorithms combine weak learners to create a strong learner?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b2258f-8d41-4901-8d95-9373a2156a5d",
   "metadata": {},
   "source": [
    "Boosting algorithms combine weak learners to create a strong learner through a process of sequential training and weighted aggregation. The key idea is to focus on instances that were misclassified or had higher errors in previous iterations, allowing subsequent weak learners to correct the mistakes made by the ensemble up to that point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bac677-cd6b-4277-957f-d2e4740606bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35612c58-2e4c-4ee5-b815-c274fa7bacfe",
   "metadata": {},
   "source": [
    "## Q7. Explain the concept of AdaBoost algorithm and its working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036f9a68-cef8-424d-bae2-1b332aca2ae5",
   "metadata": {},
   "source": [
    "* We use decision tree stump and performance of stump to predict the output.\n",
    "\n",
    "## Working\n",
    "\n",
    "Step - 1 : We will assigne equal sample weight to every instances of the dataset.\n",
    "\n",
    "Step - 2 : After that we will calculate the sum of error.Sum of error is sum of sample weights of misclassified samples.\n",
    "\n",
    "Step - 3 : We will calculate the performance of stump with help of sum of error.\n",
    "\n",
    "Step - 4 : With the help of performance of stump we will update the sample weight.\n",
    "\n",
    "Step - 5 : We have to normalize the weights.\n",
    "\n",
    "Step - 6 : We will assigne bin with the help of normalized weight.\n",
    "\n",
    "Step - 7 : Model will generate random number between 0 to 1 and in which bin that number will lie that perticular instance will send to the next model for training.\n",
    "\n",
    "Step - 8 : Formula : ∑ (lambda(Yes))\n",
    "\n",
    "           : ∑ (lambda(No))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b540ecd-3cfc-4b25-9656-93b951f05dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21997aba-ca88-4586-a85e-7ba7c48e4bc2",
   "metadata": {},
   "source": [
    "## Q8. What is the loss function used in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4b5f8d-c8c4-4a6c-8064-f7d76589aa49",
   "metadata": {},
   "source": [
    "- > Exponential loss function used in Adaptiveboost algorithm.\n",
    "\n",
    "Formuls : L( y,f( x ) ) = exp( −y⋅f( x ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b19d982-deed-47af-b496-160e134fe731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6342cb7b-da24-45b1-96f4-95768f768304",
   "metadata": {},
   "source": [
    "## Q9. How does the AdaBoost algorithm update the weights of misclassified samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33867eb8-3e69-4a2d-868b-c1224850fe30",
   "metadata": {},
   "source": [
    "* There a formula used for update the sample weight of misclassified samples.\n",
    "\n",
    "Formula : ( Weigh ) * ( e ** (Performance of Stump) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f691c969-27ea-483d-b5be-cc54f2a71076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ee8eadf-1f13-4134-a97f-b285245c153c",
   "metadata": {},
   "source": [
    "## Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d03a9f6-c88a-41fa-8bdc-6f9b71ecfd6c",
   "metadata": {},
   "source": [
    "- > Number of estimators reffers to the number of weak learners that are sequentially trained during the boosting process.\n",
    "\n",
    "- > Increasing number of estimators have both advantages and disadvantages.\n",
    "\n",
    "* Advantages\n",
    "\n",
    "The more estimators boosting tachnique has the more opportunities it has to correct misclassified samples.\n",
    "\n",
    "It performes well on new unseen data if number of estimator is high.\n",
    "\n",
    "It helps to reduce overfitting.\n",
    "\n",
    "* Disadvantages\n",
    "\n",
    "It is computationally expensive.\n",
    "\n",
    "It is time consuming.\n",
    "\n",
    "If we continously"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
